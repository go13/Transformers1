{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from t3_karpathy.compressing_transformer import CompressingAccumulativeTrainer\n",
    "from t3_karpathy.transformer_config import TransformerConfig\n",
    "from t3_karpathy.gpt_nano_dataloader import GptNanoDataloader\n",
    "from t3_karpathy.karpathy_transformer import KarpathyRunner\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n",
      "in_size=32, in_embd=256, out_size=16, out_embd=256\n",
      "in_size=16, in_embd=256, out_size=8, out_embd=256\n",
      "in_size=8, in_embd=256, out_size=4, out_embd=256\n",
      "in_size=4, in_embd=256, out_size=2, out_embd=256\n",
      "in_size=2, in_embd=256, out_size=4, out_embd=256\n",
      "in_size=4, in_embd=256, out_size=8, out_embd=256\n",
      "in_size=8, in_embd=256, out_size=16, out_embd=256\n",
      "in_size=16, in_embd=256, out_size=32, out_embd=256\n",
      "42.729802 M parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = TransformerConfig(batch_size=128, block_size=32, n_embed=256, n_head=4, n_layer=4)\n",
    "dataloader = GptNanoDataloader(config)\n",
    "\n",
    "trainer1=CompressingAccumulativeTrainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 28.6911, val loss 28.9753\n",
      "step 100: train loss 4.3690, val loss 4.4675\n",
      "step 200: train loss 4.2720, val loss 4.3599\n",
      "step 300: train loss 3.4761, val loss 3.5367\n",
      "step 400: train loss 3.3976, val loss 3.4449\n",
      "step 500: train loss 3.3491, val loss 3.3971\n",
      "step 600: train loss 3.3545, val loss 3.3954\n",
      "step 700: train loss 3.3314, val loss 3.3768\n",
      "step 800: train loss 3.3442, val loss 3.3802\n",
      "step 900: train loss 3.3393, val loss 3.3796\n",
      "step 1000: train loss 3.3296, val loss 3.3681\n",
      "step 1100: train loss 3.3259, val loss 3.3681\n",
      "step 1200: train loss 3.3209, val loss 3.3631\n",
      "step 1300: train loss 3.3296, val loss 3.3701\n",
      "step 1400: train loss 3.3297, val loss 3.3650\n",
      "step 1500: train loss 3.3272, val loss 3.3575\n",
      "step 1600: train loss 3.3230, val loss 3.3605\n",
      "step 1700: train loss 3.3201, val loss 3.3592\n",
      "step 1800: train loss 3.3117, val loss 3.3557\n",
      "step 1900: train loss 3.3182, val loss 3.3580\n",
      "step 2000: train loss 3.3200, val loss 3.3620\n",
      "step 2100: train loss 3.3253, val loss 3.3632\n",
      "step 2200: train loss 3.3171, val loss 3.3518\n",
      "step 2300: train loss 3.3202, val loss 3.3593\n",
      "step 2400: train loss 3.3256, val loss 3.3648\n",
      "step 2500: train loss 3.3168, val loss 3.3607\n",
      "step 2600: train loss 3.3249, val loss 3.3610\n",
      "step 2700: train loss 3.3225, val loss 3.3654\n",
      "step 2800: train loss 3.3227, val loss 3.3665\n",
      "step 2900: train loss 3.3160, val loss 3.3539\n",
      "step 3000: train loss 3.3119, val loss 3.3495\n",
      "step 3100: train loss 3.3145, val loss 3.3572\n",
      "step 3200: train loss 3.3143, val loss 3.3562\n",
      "step 3300: train loss 3.3218, val loss 3.3607\n",
      "step 3400: train loss 3.3191, val loss 3.3562\n",
      "step 3500: train loss 3.3195, val loss 3.3564\n",
      "step 3600: train loss 3.3183, val loss 3.3579\n",
      "step 3700: train loss 3.3136, val loss 3.3610\n",
      "step 3800: train loss 3.3159, val loss 3.3526\n",
      "step 3900: train loss 3.3158, val loss 3.3514\n",
      "step 4000: train loss 3.3183, val loss 3.3548\n",
      "step 4100: train loss 3.3114, val loss 3.3534\n",
      "step 4200: train loss 3.3129, val loss 3.3556\n",
      "step 4300: train loss 3.3184, val loss 3.3513\n",
      "step 4400: train loss 3.3183, val loss 3.3573\n",
      "step 4500: train loss 3.3127, val loss 3.3489\n",
      "step 4600: train loss 3.3146, val loss 3.3522\n",
      "step 4700: train loss 3.3135, val loss 3.3554\n",
      "step 4800: train loss 3.3153, val loss 3.3501\n",
      "step 4900: train loss 3.3192, val loss 3.3552\n",
      "step 5000: train loss 3.3133, val loss 3.3485\n",
      "step 5100: train loss 3.3127, val loss 3.3518\n",
      "step 5200: train loss 3.3175, val loss 3.3519\n",
      "step 5300: train loss 3.3182, val loss 3.3511\n",
      "step 5400: train loss 3.3181, val loss 3.3535\n",
      "step 5500: train loss 3.3117, val loss 3.3509\n",
      "step 5600: train loss 3.3195, val loss 3.3584\n",
      "step 5700: train loss 3.3159, val loss 3.3502\n",
      "step 5800: train loss 3.3182, val loss 3.3568\n",
      "step 5900: train loss 3.3118, val loss 3.3484\n",
      "step 6000: train loss 3.3151, val loss 3.3529\n",
      "step 6100: train loss 3.3119, val loss 3.3470\n",
      "step 6200: train loss 3.3085, val loss 3.3524\n",
      "step 6300: train loss 3.3120, val loss 3.3521\n",
      "step 6400: train loss 3.3154, val loss 3.3432\n",
      "step 6500: train loss 3.3120, val loss 3.3492\n",
      "step 6600: train loss 3.3103, val loss 3.3516\n",
      "step 6700: train loss 3.3129, val loss 3.3542\n",
      "step 6800: train loss 3.3146, val loss 3.3557\n",
      "step 6900: train loss 3.3153, val loss 3.3537\n",
      "step 7000: train loss 3.3133, val loss 3.3497\n",
      "step 7100: train loss 3.3159, val loss 3.3484\n",
      "step 7200: train loss 3.3124, val loss 3.3519\n",
      "step 7300: train loss 3.3115, val loss 3.3493\n",
      "step 7400: train loss 3.3123, val loss 3.3487\n",
      "step 7500: train loss 3.3146, val loss 3.3499\n",
      "step 7600: train loss 3.3101, val loss 3.3515\n",
      "step 7700: train loss 3.3134, val loss 3.3514\n",
      "step 7800: train loss 3.3101, val loss 3.3509\n",
      "step 7900: train loss 3.3139, val loss 3.3459\n",
      "step 8000: train loss 3.3122, val loss 3.3496\n",
      "step 8100: train loss 3.3085, val loss 3.3480\n",
      "step 8200: train loss 3.3130, val loss 3.3457\n",
      "step 8300: train loss 3.3130, val loss 3.3498\n",
      "step 8400: train loss 3.3108, val loss 3.3492\n",
      "step 8500: train loss 3.3112, val loss 3.3595\n",
      "step 8600: train loss 3.3109, val loss 3.3490\n",
      "step 8700: train loss 3.3115, val loss 3.3496\n",
      "step 8800: train loss 3.3101, val loss 3.3513\n",
      "step 8900: train loss 3.3144, val loss 3.3494\n",
      "step 9000: train loss 3.3090, val loss 3.3460\n",
      "step 9100: train loss 3.3131, val loss 3.3500\n",
      "step 9200: train loss 3.3102, val loss 3.3452\n",
      "step 9300: train loss 3.3104, val loss 3.3533\n",
      "step 9400: train loss 3.3137, val loss 3.3460\n",
      "step 9500: train loss 3.3087, val loss 3.3493\n",
      "step 9600: train loss 3.3149, val loss 3.3500\n",
      "step 9700: train loss 3.3115, val loss 3.3493\n",
      "step 9800: train loss 3.3120, val loss 3.3487\n",
      "step 9900: train loss 3.3094, val loss 3.3476\n",
      "step 10000: train loss 3.3142, val loss 3.3476\n",
      "step 10100: train loss 3.3149, val loss 3.3514\n",
      "step 10200: train loss 3.3117, val loss 3.3475\n",
      "step 10300: train loss 3.3079, val loss 3.3527\n",
      "step 10400: train loss 3.3092, val loss 3.3478\n",
      "step 10500: train loss 3.3096, val loss 3.3501\n",
      "step 10600: train loss 3.3114, val loss 3.3473\n",
      "step 10700: train loss 3.3100, val loss 3.3493\n",
      "step 10800: train loss 3.3077, val loss 3.3474\n",
      "step 10900: train loss 3.3093, val loss 3.3518\n",
      "step 11000: train loss 3.3106, val loss 3.3454\n",
      "step 11100: train loss 3.3116, val loss 3.3485\n",
      "step 11200: train loss 3.3077, val loss 3.3489\n",
      "step 11300: train loss 3.3123, val loss 3.3485\n",
      "step 11400: train loss 3.3095, val loss 3.3487\n",
      "step 11500: train loss 3.3107, val loss 3.3515\n",
      "step 11600: train loss 3.3105, val loss 3.3518\n",
      "step 11700: train loss 3.3124, val loss 3.3493\n",
      "step 11800: train loss 3.3095, val loss 3.3479\n",
      "step 11900: train loss 3.3094, val loss 3.3465\n",
      "step 12000: train loss 3.3102, val loss 3.3480\n",
      "step 12100: train loss 3.3095, val loss 3.3482\n",
      "step 12200: train loss 3.3081, val loss 3.3496\n",
      "step 12300: train loss 3.3100, val loss 3.3481\n",
      "step 12400: train loss 3.3049, val loss 3.3471\n",
      "step 12500: train loss 3.3107, val loss 3.3470\n",
      "step 12600: train loss 3.3133, val loss 3.3458\n",
      "step 12700: train loss 3.3123, val loss 3.3471\n",
      "step 12800: train loss 3.3106, val loss 3.3468\n",
      "step 12900: train loss 3.3118, val loss 3.3456\n",
      "step 13000: train loss 3.3120, val loss 3.3505\n",
      "step 13100: train loss 3.3102, val loss 3.3508\n",
      "step 13200: train loss 3.3144, val loss 3.3532\n",
      "step 13300: train loss 3.3109, val loss 3.3499\n",
      "step 13400: train loss 3.3090, val loss 3.3488\n",
      "step 13500: train loss 3.3077, val loss 3.3467\n",
      "step 13600: train loss 3.3128, val loss 3.3468\n",
      "step 13700: train loss 3.3106, val loss 3.3506\n",
      "step 13800: train loss 3.3113, val loss 3.3488\n",
      "step 13900: train loss 3.3116, val loss 3.3459\n",
      "step 14000: train loss 3.3087, val loss 3.3465\n",
      "step 14100: train loss 3.3082, val loss 3.3475\n",
      "step 14200: train loss 3.3124, val loss 3.3500\n",
      "step 14300: train loss 3.3088, val loss 3.3484\n",
      "step 14400: train loss 3.3110, val loss 3.3506\n",
      "step 14500: train loss 3.3115, val loss 3.3474\n",
      "step 14600: train loss 3.3119, val loss 3.3467\n",
      "step 14700: train loss 3.3105, val loss 3.3503\n",
      "step 14800: train loss 3.3114, val loss 3.3540\n",
      "step 14900: train loss 3.3098, val loss 3.3465\n",
      "step 15000: train loss 3.3128, val loss 3.3521\n",
      "step 15100: train loss 3.3087, val loss 3.3526\n",
      "step 15200: train loss 3.3206, val loss 3.3623\n",
      "step 15300: train loss 3.3099, val loss 3.3452\n",
      "step 15400: train loss 3.3139, val loss 3.3506\n",
      "step 15500: train loss 3.3093, val loss 3.3484\n",
      "step 15600: train loss 3.3135, val loss 3.3494\n",
      "step 15700: train loss 3.3083, val loss 3.3484\n",
      "step 15800: train loss 3.3108, val loss 3.3438\n",
      "step 15900: train loss 3.3083, val loss 3.3473\n",
      "step 16000: train loss 3.3170, val loss 3.3500\n",
      "step 16100: train loss 3.3106, val loss 3.3485\n",
      "step 16200: train loss 3.3094, val loss 3.3492\n",
      "step 16300: train loss 3.3095, val loss 3.3501\n",
      "step 16400: train loss 3.3084, val loss 3.3521\n",
      "step 16500: train loss 3.3106, val loss 3.3498\n",
      "step 16600: train loss 3.3091, val loss 3.3493\n",
      "step 16700: train loss 3.3116, val loss 3.3522\n",
      "step 16800: train loss 3.3087, val loss 3.3500\n",
      "step 16900: train loss 3.3086, val loss 3.3471\n",
      "step 17000: train loss 3.3134, val loss 3.3529\n",
      "step 17100: train loss 3.3102, val loss 3.3472\n",
      "step 17200: train loss 3.3102, val loss 3.3452\n",
      "step 17300: train loss 3.3104, val loss 3.3465\n",
      "step 17400: train loss 3.3061, val loss 3.3501\n",
      "step 17500: train loss 3.3086, val loss 3.3475\n",
      "step 17600: train loss 3.3120, val loss 3.3445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17700: train loss 3.3071, val loss 3.3495\n",
      "step 17800: train loss 3.3079, val loss 3.3468\n",
      "step 17900: train loss 3.3139, val loss 3.3479\n",
      "step 18000: train loss 3.3102, val loss 3.3471\n",
      "step 18100: train loss 3.3093, val loss 3.3498\n",
      "step 18200: train loss 3.3076, val loss 3.3444\n",
      "step 18300: train loss 3.3081, val loss 3.3495\n",
      "step 18400: train loss 3.3091, val loss 3.3492\n",
      "step 18500: train loss 3.3097, val loss 3.3460\n",
      "step 18600: train loss 3.3088, val loss 3.3543\n",
      "step 18700: train loss 3.3088, val loss 3.3500\n",
      "step 18800: train loss 3.3057, val loss 3.3496\n",
      "step 18900: train loss 3.3116, val loss 3.3524\n",
      "step 19000: train loss 3.3081, val loss 3.3468\n",
      "step 19100: train loss 3.3094, val loss 3.3457\n",
      "step 19200: train loss 3.3115, val loss 3.3458\n",
      "step 19300: train loss 3.3129, val loss 3.3504\n",
      "step 19400: train loss 3.3091, val loss 3.3483\n",
      "step 19500: train loss 3.3086, val loss 3.3488\n",
      "step 19600: train loss 3.3104, val loss 3.3486\n",
      "step 19700: train loss 3.3132, val loss 3.3477\n",
      "step 19800: train loss 3.3082, val loss 3.3461\n",
      "step 19900: train loss 3.3108, val loss 3.3511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer1.train_iterate(20000, dataloader.get_train_batch, dataloader.get_val_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26056/3934323622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_codec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataloader.token_codec.decode(trainer1.generate(torch.zeros((1, 1), dtype=torch.long, device=device), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dataloader.token_codec.decode(trainer1.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
