{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from t3_karpathy.compressing_transformer import CompressingAccumulativeTrainer\n",
    "from t3_karpathy.transformer_config import TransformerConfig\n",
    "from t3_karpathy.gpt_nano_dataloader import GptNanoDataloader\n",
    "from t3_karpathy.karpathy_transformer import KarpathyRunner\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig()\n",
    "dataloader = GptNanoDataloader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.105514 M parameters\n",
      "0.105514 M parameters\n"
     ]
    }
   ],
   "source": [
    "trainer1=CompressingAccumulativeTrainer(config)\n",
    "#trainer1=KarpathyRunner(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 15.2542, val loss 15.1546\n",
      "step 100: train loss 3.4527, val loss 3.5084\n",
      "step 200: train loss 3.0645, val loss 3.0960\n",
      "step 300: train loss 2.8779, val loss 2.8937\n",
      "step 400: train loss 2.7379, val loss 2.7583\n",
      "step 500: train loss 2.6531, val loss 2.6520\n",
      "step 600: train loss 2.6077, val loss 2.6145\n",
      "step 700: train loss 2.5623, val loss 2.5647\n",
      "step 800: train loss 2.5285, val loss 2.5404\n",
      "step 900: train loss 2.5014, val loss 2.5087\n",
      "step 1000: train loss 2.4639, val loss 2.4555\n",
      "step 1100: train loss 2.4341, val loss 2.4393\n",
      "step 1200: train loss 2.4112, val loss 2.4069\n",
      "step 1300: train loss 2.3943, val loss 2.3858\n",
      "step 1400: train loss 2.3795, val loss 2.3843\n",
      "step 1500: train loss 2.3517, val loss 2.3578\n",
      "step 1600: train loss 2.3276, val loss 2.3207\n",
      "step 1700: train loss 2.3285, val loss 2.3242\n",
      "step 1800: train loss 2.3140, val loss 2.3158\n",
      "step 1900: train loss 2.2845, val loss 2.2883\n",
      "step 2000: train loss 2.2800, val loss 2.2783\n",
      "step 2100: train loss 2.2680, val loss 2.2728\n",
      "step 2200: train loss 2.2507, val loss 2.2794\n",
      "step 2300: train loss 2.2563, val loss 2.2627\n",
      "step 2400: train loss 2.2340, val loss 2.2475\n",
      "step 2500: train loss 2.2214, val loss 2.2356\n",
      "step 2600: train loss 2.1992, val loss 2.2142\n",
      "step 2700: train loss 2.2023, val loss 2.2160\n",
      "step 2800: train loss 2.1864, val loss 2.1866\n",
      "step 2900: train loss 2.1757, val loss 2.2033\n",
      "step 3000: train loss 2.1618, val loss 2.1830\n",
      "step 3100: train loss 2.1523, val loss 2.1726\n",
      "step 3200: train loss 2.1470, val loss 2.1621\n",
      "step 3300: train loss 2.1328, val loss 2.1667\n",
      "step 3400: train loss 2.1269, val loss 2.1523\n",
      "step 3500: train loss 2.1300, val loss 2.1609\n",
      "step 3600: train loss 2.1195, val loss 2.1525\n",
      "step 3700: train loss 2.1051, val loss 2.1348\n",
      "step 3800: train loss 2.0966, val loss 2.1393\n",
      "step 3900: train loss 2.0810, val loss 2.1179\n",
      "step 4000: train loss 2.0812, val loss 2.1144\n",
      "step 4100: train loss 2.0766, val loss 2.1108\n",
      "step 4200: train loss 2.0591, val loss 2.1055\n",
      "step 4300: train loss 2.0538, val loss 2.1108\n",
      "step 4400: train loss 2.0545, val loss 2.1123\n",
      "step 4500: train loss 2.0329, val loss 2.0809\n",
      "step 4600: train loss 2.0248, val loss 2.0840\n",
      "step 4700: train loss 2.0417, val loss 2.0951\n",
      "step 4800: train loss 2.0192, val loss 2.0764\n",
      "step 4900: train loss 2.0216, val loss 2.0765\n"
     ]
    }
   ],
   "source": [
    "trainer1.train_iterate(5000, dataloader.get_train_batch, dataloader.get_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "725614q434614M14Y1lKYP417lKY3Krv6v6v665656v6v666656v6h565151514v1P11141561P15656v656v656v6z4v66v614v\n"
     ]
    }
   ],
   "source": [
    "print(dataloader.token_codec.decode(trainer1.generate(torch.zeros((1, 1), dtype=torch.long, device=device), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "74x1cj664417q4nW?1lK4ll1761K3Kr56v6v656v66656v6v65655653556v651P1115565611415561v156v65f156v656656v6565656w1lul156651P156151P1156641P17l1v6v6e4v6v4v6hj4?4?6v6v6v66v6v6v65Hz4?15656v6565656h-w15656565656v6535656v153h5v6zMh1P8h-56v1156Bw11i66156z156v66566v65661P141P151K1565656?117566v6v6v6v6v6?I5656v6v1l11K11?155151565656566v6v6v6656?1l14156v65565565511P15666h56v6v6v6v6z156565651?15656w1P1P156?h5656v1565156v6v66v65w156656v156w1556v6v1P151566v656v6w156?15656w1l155665651565h5176h1?656v66vz15656v156v6656vz15656553565656h566h515556v15151v6566vhh56v66656?1K6141v1417v615W51156v656v6v1l6v6v656v6v11561l141v65656656v11l17l176v65656v66e?65654z4156v56w155156v656566v?1415656565655656v156532w1765Eq4151765665v61l2w14v6vq4q4156v6v6v6?1l15656v6v6v6566v6v?1565615656v65w115111765656515651P1P1ljz41565v6v656v6v6v6v6Iv656v6561K15114151511156566615v6v1v6v656656v14v65656?176517l1?156v1v1565615656156561lK156ef11l87v6566517v1566v6656v656v65656h155656565656541P1565665h566565656v6565v6556565651561P156h15w15651l166v1l1P1l2566v66v6v6v6656v4176v656v656v156h515356515617lj3256v65665656v65656v6v65W53565656z1P11555656565156v614ljv156h56v6v6Nz1511l156565656v6v6v1515656v656v6v1415656h15156v651?17j415656565656v6565655156565656565111565611P156115665141515665656565156v6656v32565656v6h-535653q4vu56156v15366565656v6h66h56566v61l6v?1P1K71156v1156561v156156566v1lj566v1?6655661561565W55566151565656v651565656v?1156v11K153q4q41P156vv?1lj6v6666v66v6v6v656656v6N4v655656z15151P15656v156565v656566v615666?141411P1511P1l2v6656v656556v656v6v6v6566565653565h5W1565656w156v65656565656h51565656v6556565656556517v1566z1l156565w1515651v1?1w1765656614q4v65665w156v1l6v66v6656?1565656561l656z41565655656515656M1v11P1l11z156v65651P15656v656v6v6v66561w1P2w1?141P1l2j56566v6v6v656v6v656v65656v6656v15651551156511P15656v156vL1l156v1l2w1?Iz415656vv6v66v656v656v15w1w156?1v1l1P1565W11K1v65666vv6v6?1v6v66v6?1v6?141565156565656561766v6565156v656vz1?11P1565656v11156v6v11l4v65656v66v65vv6M14156565651111l1154156vv656v6v6v66v1l256v14156561v17z156\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dataloader.token_codec.decode(trainer1.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}